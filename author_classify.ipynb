{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import movie_reviews\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords listing\n",
    "## Extend the list of stopwords by analyzing your language transiliterated in english (ex-hindi, bengali, telegu, tamil etc) for better precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Transliterated Hindi Words\n",
    "stopwords.extend(['hai','nhi','mai','toh','ho','kya','na','ka','hi','ki','tum','nahi','bhi',\n",
    "                  'haan','se','ke','tha','k','aur','rhe','ko','rhi','main','mujhe','abhi','voh','b',\n",
    "                  'hun','thi','hain','ek','kar','rha','e','hoga','kal','lekin','tumne',\n",
    "                  'hua','arey','pr','koi','liye','hum','maine','gaya','accha','aa','tumhe','mera',\n",
    "                  'kuch','yeh','hota','u','ye','time','bohot','er','tumhara','lab',\n",
    "                  'kyun','kr','class','fir','sir','hu','gayi','karna','chahiye','acha','n','jo','nt'])\n",
    "\n",
    "le = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):   \n",
    "    \n",
    "    tokens=word_tokenize(text.lower())\n",
    "    \n",
    "    punctuation=re.compile('[^a-zA-Z]*')    \n",
    "    post_punctutation=([punctuation.sub(\"\", word) for word in tokens])\n",
    "      \n",
    "    stem_token=[le.lemmatize(word) for word in post_punctutation if word not in stopwords]    \n",
    "    return \" \".join(stem_token)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your name mentioned in your whatsapp account at the time of exporting chats\n",
    "my_name = 'Sayan Dey'\n",
    "\n",
    "df = read_csv('./dataset/whatsapp_df.csv')\n",
    "\n",
    "# preprocess the messages\n",
    "df['Message']=df['Message'].transform(lambda x : preprocess(x))\n",
    "\n",
    "# drop rows where message is too small\n",
    "message_threshold_size = 4\n",
    "df=df[df['Message'].apply(lambda x : len(word_tokenize(x)))>=message_threshold_size]\n",
    "\n",
    "# drop your own messages as it may overshadow others' messages due to high occurence\n",
    "# comment it out if you want to include your chats\n",
    "df=df[df['Author'] != my_name]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the words and label the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vect=TfidfVectorizer()\n",
    "X=vect.fit_transform(df['Message'])\n",
    "dtm_df=DataFrame(X.toarray(), columns=vect.get_feature_names()) \n",
    "\n",
    "label=LabelEncoder()\n",
    "dtm_df['Author']=label.fit_transform(df['Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(dtm_df.drop(['Author'],axis=1), dtm_df['Author'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr=LogisticRegression(C=150)\n",
    "clf_lr.fit(x_train,y_train)\n",
    "pred=clf_lr.predict(x_test)\n",
    "print(\"The accuracy of Logistic Regression :\" , accuracy_score(pred,y_test)) \n",
    "print(\"The classification report is : \\n\"+classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental sentence-author prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the sentence string for more experimentations\n",
    "sentence = 'Do we have class today?'\n",
    "\n",
    "label.inverse_transform(clf_lr.predict(vect.transform([sentence]).toarray()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
