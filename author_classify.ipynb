{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import movie_reviews\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords listing\n",
    "## Extend the list of stopwords by analyzing your language transiliterated in english (ex-hindi, bengali, telegu, tamil etc) for better precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Transliterated Hindi Words\n",
    "stopwords.extend(['hai','nhi','mai','toh','ho','kya','na','ka','hi','ki','tum','nahi','bhi',\n",
    "                  'haan','se','ke','tha','k','aur','rhe','ko','rhi','main','mujhe','abhi','voh','b',\n",
    "                  'hun','thi','hain','ek','kar','rha','e','hoga','kal','lekin','tumne',\n",
    "                  'hua','arey','pr','koi','liye','hum','maine','gaya','accha','aa','tumhe','mera',\n",
    "                  'kuch','yeh','hota','u','ye','time','bohot','er','tumhara','lab',\n",
    "                  'kyun','kr','class','fir','sir','hu','gayi','karna','chahiye','acha','n','jo','nt'])\n",
    "\n",
    "le = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):   \n",
    "    \n",
    "    tokens=word_tokenize(text.lower())\n",
    "    \n",
    "    punctuation=re.compile('[^a-zA-Z]*')    \n",
    "    post_punctutation=([punctuation.sub(\"\", word) for word in tokens])\n",
    "      \n",
    "    stem_token=[le.lemmatize(word) for word in post_punctutation if word not in stopwords]    \n",
    "    return \" \".join(stem_token)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aitihya Dey</td>\n",
       "      <td>hay bal eta abar kothay lekha ache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aitihya Dey</td>\n",
       "      <td>bal email pathiechis  link tar kichui kora jae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Aitihya Dey</td>\n",
       "      <td>bal koto bari jai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aitihya Dey</td>\n",
       "      <td>sala ami vblm amr kaj pore gelo tr sathe dekha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Aitihya Dey</td>\n",
       "      <td>hazarduary te hazar bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Author                                            Message\n",
       "1   Aitihya Dey                 hay bal eta abar kothay lekha ache\n",
       "4   Aitihya Dey     bal email pathiechis  link tar kichui kora jae\n",
       "22  Aitihya Dey                                  bal koto bari jai\n",
       "24  Aitihya Dey  sala ami vblm amr kaj pore gelo tr sathe dekha...\n",
       "31  Aitihya Dey                            hazarduary te hazar bar"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your name mentioned in your whatsapp account at the time of exporting chats\n",
    "my_name = 'Sayan Dey'\n",
    "\n",
    "df = read_csv('./dataset/whatsapp_df.csv')\n",
    "\n",
    "# preprocess the messages\n",
    "df['Message']=df['Message'].transform(lambda x : preprocess(x))\n",
    "\n",
    "# drop rows where message is too small\n",
    "message_threshold_size = 4\n",
    "df=df[df['Message'].apply(lambda x : len(word_tokenize(x)))>=message_threshold_size]\n",
    "\n",
    "# drop your own messages as it may overshadow others' messages due to high occurence\n",
    "# comment it out if you want to include your chats\n",
    "df=df[df['Author'] != my_name]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the words and label the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vect=TfidfVectorizer()\n",
    "X=vect.fit_transform(df['Message'])\n",
    "dtm_df=DataFrame(X.toarray(), columns=vect.get_feature_names()) \n",
    "\n",
    "label=LabelEncoder()\n",
    "dtm_df['Author']=label.fit_transform(df['Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(dtm_df.drop(['Author'],axis=1), dtm_df['Author'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression : 0.9568151147098516\n",
      "The classification report is : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       127\n",
      "           1       0.44      0.89      0.59        18\n",
      "           2       0.86      1.00      0.92        74\n",
      "           3       1.00      0.93      0.96       276\n",
      "           4       1.00      0.96      0.98       246\n",
      "\n",
      "    accuracy                           0.96       741\n",
      "   macro avg       0.86      0.96      0.89       741\n",
      "weighted avg       0.97      0.96      0.96       741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/sayan/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_lr=LogisticRegression(C=150)\n",
    "clf_lr.fit(x_train,y_train)\n",
    "pred=clf_lr.predict(x_test)\n",
    "print(\"The accuracy of Logistic Regression :\" , accuracy_score(pred,y_test)) \n",
    "print(\"The classification report is : \\n\"+classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental sentence-author prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Swayam'], dtype=object)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Do we have class today?'\n",
    "\n",
    "label.inverse_transform(clf_lr.predict(vect.transform([sentence]).toarray()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
